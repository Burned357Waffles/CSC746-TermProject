# 3D N-Body Problem in CPU OpenMP and GPU CUDA

This directory contains the two different implementations of a 3d N-Body problem simulator. The two 
implementatinos are:

* CPU only in C++, with your added OpenMP parallelism
* GPU in CUDA

# Build instructions - general

This code requires use of the Nvidia compilers as follows:

    module load PrgEnv-nvidia
    export CC=cc
    export CXX=CC 

Then, once your environment is set up, then:

    mkdir build  
    cd build  
    cmake ../  -Wno-dev
    make

It is OK to do builds on the login node once you have set up the environment above.  
All code should be executed on a GPU node.

# Run instructions - sobel_gpu

Please use the following command to run the nbody_gpu code from the SCRATCH directory of Perlmutter while on a GPU node:

    cd $SCRATCH
    salloc -C gpu -t 30 -c 1 -G 1 -q interactive -A m3930
    ./nbody_gpu <number_of_bodies> <record_histories> <timestep_modifier> <final_time_modifier> <threads_per_block> <num_blocks>

    * Number of bodies should be a postive integer if you want random bodies. If you want a model of our solar system, use -1
    * Record histories is a boolean
        - 0 will not record velocity and position history
        - 1 will record velocity and position history
    * Timestep modifier will scale the timestep multiplcitively. For example, a value of 1 will be a one hour timestep, a value of 2 will be 2 hours, a value of 0.5 will be 30 minutes.
    * Final time will scale the total simulation time multiplicitively. For example, a value of 1 will be 1 Earth year, while a value of 10 will be 10 earth years.
    * Threads per block should be a multiple of 32
    * Number of blocks should be scaled to the problem size and threads per block

    
These arguments are required to run the code. Here are some example usages:
    
    ~> ./nbody_gpu 256 0 1 10 32 8
    ~> ./nbody_gpu -1 1 7 300 32 1

# Run instructions - sobel_cpu

Please use the following command to run the nbody_cpu code from the SCRATCH directory of Perlmutter while on a GPU node:

    cd $SCRATCH
    salloc -C gpu -t 30 -c 1 -G 1 -q interactive -A m3930
    OMP_NUM_THREADS=<num_threads> ./nbody_gpu <number_of_bodies> <record_histories> <timestep_modifier> <final_time_modifier>

    * We are using OMP_NUM_THREADS in the commandline to set the number of threads used
    * Number of bodies should be a postive integer if you want random bodies. If you want a model of our solar system, use -1
    * Record histories is a boolean
        - 0 will not record velocity and position history
        - 1 will record velocity and position history
    * Timestep modifier will scale the timestep multiplcitively. For example, a value of 1 will be a one hour timestep, a value of 2 will be 2 hours, a value of 0.5 will be 30 minutes.
    * Final time will scale the total simulation time multiplicitively. For example, a value of 1 will be 1 Earth year, while a value of 10 will be 10 earth years.
    

    
These arguments are required to run the code. Here are some example usages:
    
    ~> OMP_NUM_THREADS=1 ./nbody_cpu 256 0 1 10 
    ~> OMP_NUM_THREADS=64 ./nbody_cpu -1 1 7 300 


# python display scripts

* nbody-simulator-csv.py - a python script to display the entire position history of all bodies at once. You can pan around and zoom to get a better view.

* nbody-animator-csv.py -a python script to display an animation of the entire position history of all bodies at once. You can pan around and zoom to get a better view. This has the option to either display the plot, or write it to a gif. To switch between these, please comment/uncomment the marked lines at the bottom of the file.


Usage:  

    For both files, you will need to change the "filename" variable (at line 8/9) to be the correct path of the positions.csv file generated by the code.

    To run the files, run these commands:
        ~> python .\nbody-simulator-csv.py
        ~> python .\nbody-animator-csv.py


# eof
